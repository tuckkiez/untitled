#!/usr/bin/env python3
"""
Goal5.co Match Analysis Scraper
‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏á‡∏≤‡∏ô‡∏à‡∏≤‡∏Å goal5.co
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime
import time

class Goal5Scraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def scrape_match_analysis(self, match_id):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏°‡∏ó‡∏ä‡πå"""
        url = f"https://goal5.co/analyse/?id={match_id}"
        
        print(f"üîç Scraping match analysis from: {url}")
        
        try:
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                print(f"‚úÖ Successfully loaded page")
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏°‡∏ó‡∏ä‡πå
                match_info = self.extract_match_info(soup)
                
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
                statistics = self.extract_statistics(soup)
                
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ü‡∏≠‡∏£‡πå‡∏°
                form_data = self.extract_form_data(soup)
                
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
                predictions = self.extract_predictions(soup)
                
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πà‡∏≠‡∏£‡∏≠‡∏á
                odds_data = self.extract_odds_data(soup)
                
                return {
                    'match_info': match_info,
                    'statistics': statistics,
                    'form_data': form_data,
                    'predictions': predictions,
                    'odds_data': odds_data,
                    'scraped_at': datetime.now().isoformat(),
                    'source_url': url
                }
            else:
                print(f"‚ùå Failed to load page: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"‚ùå Error scraping: {e}")
            return None
    
    def extract_match_info(self, soup):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏°‡∏ó‡∏ä‡πå"""
        match_info = {}
        
        try:
            # ‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡∏°
            team_elements = soup.find_all(['h1', 'h2', 'h3'], class_=re.compile(r'team|match'))
            if not team_elements:
                team_elements = soup.find_all('div', class_=re.compile(r'team|match'))
            
            # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ "vs" ‡∏´‡∏£‡∏∑‡∏≠ "-"
            match_title = soup.find('title')
            if match_title:
                title_text = match_title.get_text()
                match_info['page_title'] = title_text
                
                # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡∏°
                if ' vs ' in title_text:
                    teams = title_text.split(' vs ')
                    if len(teams) >= 2:
                        match_info['home_team'] = teams[0].strip()
                        match_info['away_team'] = teams[1].split(' - ')[0].strip()
                elif ' - ' in title_text:
                    teams = title_text.split(' - ')
                    if len(teams) >= 2:
                        match_info['home_team'] = teams[0].strip()
                        match_info['away_team'] = teams[1].strip()
            
            # ‡∏´‡∏≤‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤
            date_elements = soup.find_all(text=re.compile(r'\d{1,2}/\d{1,2}/\d{4}|\d{4}-\d{2}-\d{2}'))
            if date_elements:
                match_info['date_found'] = [elem.strip() for elem in date_elements[:3]]
            
            # ‡∏´‡∏≤‡∏•‡∏µ‡∏Å
            league_elements = soup.find_all(text=re.compile(r'Premier|Liga|League|Division|Championship'))
            if league_elements:
                match_info['league_mentions'] = [elem.strip() for elem in league_elements[:3]]
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error extracting match info: {e}")
        
        return match_info
    
    def extract_statistics(self, soup):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥"""
        statistics = {}
        
        try:
            # ‡∏´‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
            tables = soup.find_all('table')
            for i, table in enumerate(tables):
                rows = table.find_all('tr')
                table_data = []
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if cells:
                        row_data = [cell.get_text().strip() for cell in cells]
                        table_data.append(row_data)
                
                if table_data:
                    statistics[f'table_{i+1}'] = table_data
            
            # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô
            stat_divs = soup.find_all('div', class_=re.compile(r'stat|score|percent'))
            stat_data = []
            
            for div in stat_divs:
                text = div.get_text().strip()
                if text and len(text) < 100:  # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô
                    stat_data.append(text)
            
            if stat_data:
                statistics['stat_elements'] = stat_data[:20]  # ‡πÄ‡∏Å‡πá‡∏ö 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏Å
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error extracting statistics: {e}")
        
        return statistics
    
    def extract_form_data(self, soup):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ü‡∏≠‡∏£‡πå‡∏°"""
        form_data = {}
        
        try:
            # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            form_elements = soup.find_all(text=re.compile(r'W|L|D'))
            form_matches = []
            
            for elem in form_elements:
                text = elem.strip()
                # ‡∏´‡∏≤‡πÅ‡∏û‡∏ó‡πÄ‡∏ó‡∏¥‡∏£‡πå‡∏ô‡∏ü‡∏≠‡∏£‡πå‡∏° ‡πÄ‡∏ä‡πà‡∏ô WWLDW
                if re.match(r'^[WLD]{3,10}$', text):
                    form_matches.append(text)
            
            if form_matches:
                form_data['form_patterns'] = form_matches[:10]
            
            # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            score_elements = soup.find_all(text=re.compile(r'\d+-\d+'))
            scores = []
            
            for elem in score_elements:
                text = elem.strip()
                if re.match(r'^\d+-\d+$', text):
                    scores.append(text)
            
            if scores:
                form_data['recent_scores'] = scores[:10]
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error extracting form data: {e}")
        
        return form_data
    
    def extract_predictions(self, soup):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢"""
        predictions = {}
        
        try:
            # ‡∏´‡∏≤‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
            percentage_elements = soup.find_all(text=re.compile(r'\d+%'))
            percentages = []
            
            for elem in percentage_elements:
                text = elem.strip()
                if re.match(r'^\d+%$', text):
                    percentages.append(text)
            
            if percentages:
                predictions['percentages'] = percentages[:10]
            
            # ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
            prediction_keywords = ['Home Win', 'Away Win', 'Draw', 'Over', 'Under', 'BTTS', 'Clean Sheet']
            prediction_elements = []
            
            for keyword in prediction_keywords:
                elements = soup.find_all(text=re.compile(keyword, re.IGNORECASE))
                for elem in elements:
                    text = elem.strip()
                    if keyword.lower() in text.lower():
                        prediction_elements.append(text)
            
            if prediction_elements:
                predictions['prediction_text'] = prediction_elements[:10]
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error extracting predictions: {e}")
        
        return predictions
    
    def extract_odds_data(self, soup):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πà‡∏≠‡∏£‡∏≠‡∏á"""
        odds_data = {}
        
        try:
            # ‡∏´‡∏≤‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πà‡∏≠‡∏£‡∏≠‡∏á‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°
            decimal_odds = soup.find_all(text=re.compile(r'\d+\.\d{2}'))
            odds_values = []
            
            for elem in decimal_odds:
                text = elem.strip()
                try:
                    value = float(text)
                    if 1.0 <= value <= 50.0:  # ‡∏ä‡πà‡∏ß‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•
                        odds_values.append(value)
                except:
                    continue
            
            if odds_values:
                odds_data['decimal_odds'] = odds_values[:15]
            
            # ‡∏´‡∏≤‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πà‡∏≠‡∏£‡∏≠‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏®‡∏©‡∏™‡πà‡∏ß‡∏ô
            fraction_odds = soup.find_all(text=re.compile(r'\d+/\d+'))
            fractions = []
            
            for elem in fraction_odds:
                text = elem.strip()
                if re.match(r'^\d+/\d+$', text):
                    fractions.append(text)
            
            if fractions:
                odds_data['fractional_odds'] = fractions[:10]
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error extracting odds data: {e}")
        
        return odds_data
    
    def analyze_scraped_data(self, data):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤"""
        analysis = {
            'summary': {},
            'insights': [],
            'recommendations': []
        }
        
        if not data:
            return analysis
        
        try:
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            match_info = data.get('match_info', {})
            if 'home_team' in match_info and 'away_team' in match_info:
                analysis['summary']['match'] = f"{match_info['home_team']} vs {match_info['away_team']}"
                analysis['insights'].append(f"‚úÖ Match identified: {analysis['summary']['match']}")
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
            statistics = data.get('statistics', {})
            if statistics:
                total_stats = sum(len(v) for v in statistics.values() if isinstance(v, list))
                analysis['summary']['statistics_found'] = total_stats
                analysis['insights'].append(f"üìä Found {total_stats} statistical data points")
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ü‡∏≠‡∏£‡πå‡∏°
            form_data = data.get('form_data', {})
            if form_data.get('form_patterns'):
                analysis['summary']['form_patterns'] = len(form_data['form_patterns'])
                analysis['insights'].append(f"üìà Found {len(form_data['form_patterns'])} form patterns")
            
            if form_data.get('recent_scores'):
                analysis['summary']['recent_scores'] = len(form_data['recent_scores'])
                analysis['insights'].append(f"‚öΩ Found {len(form_data['recent_scores'])} recent scores")
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
            predictions = data.get('predictions', {})
            if predictions.get('percentages'):
                analysis['summary']['prediction_percentages'] = len(predictions['percentages'])
                analysis['insights'].append(f"üéØ Found {len(predictions['percentages'])} prediction percentages")
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πà‡∏≠‡∏£‡∏≠‡∏á
            odds_data = data.get('odds_data', {})
            if odds_data.get('decimal_odds'):
                analysis['summary']['odds_values'] = len(odds_data['decimal_odds'])
                analysis['insights'].append(f"üí∞ Found {len(odds_data['decimal_odds'])} odds values")
            
            # ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
            if total_stats > 10:
                analysis['recommendations'].append("‚úÖ Sufficient statistical data for analysis")
            else:
                analysis['recommendations'].append("‚ö†Ô∏è Limited statistical data - may need additional sources")
            
            if form_data:
                analysis['recommendations'].append("‚úÖ Form data available for trend analysis")
            
            if predictions:
                analysis['recommendations'].append("‚úÖ Prediction data can be used for comparison")
            
            if odds_data:
                analysis['recommendations'].append("‚úÖ Odds data available for value betting analysis")
            
        except Exception as e:
            analysis['insights'].append(f"‚ùå Error in analysis: {e}")
        
        return analysis

def main():
    print("üîç Goal5.co Match Analysis Scraper")
    print("=" * 50)
    
    match_id = "4866151"
    scraper = Goal5Scraper()
    
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    print(f"üì° Scraping match ID: {match_id}")
    scraped_data = scraper.scrape_match_analysis(match_id)
    
    if scraped_data:
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö
        with open(f'goal5_match_{match_id}_raw.json', 'w', encoding='utf-8') as f:
            json.dump(scraped_data, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Raw data saved to goal5_match_{match_id}_raw.json")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        print(f"\nüß† Analyzing scraped data...")
        analysis = scraper.analyze_scraped_data(scraped_data)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
        print(f"\nüìä ANALYSIS RESULTS")
        print("=" * 50)
        
        # ‡∏™‡∏£‡∏∏‡∏õ
        summary = analysis.get('summary', {})
        if summary:
            print(f"üìã Summary:")
            for key, value in summary.items():
                print(f"   {key}: {value}")
        
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å
        insights = analysis.get('insights', [])
        if insights:
            print(f"\nüí° Insights:")
            for insight in insights:
                print(f"   {insight}")
        
        # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        recommendations = analysis.get('recommendations', [])
        if recommendations:
            print(f"\nüéØ Recommendations:")
            for rec in recommendations:
                print(f"   {rec}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        match_info = scraped_data.get('match_info', {})
        if match_info:
            print(f"\nüèÜ MATCH INFORMATION")
            print("-" * 30)
            for key, value in match_info.items():
                if isinstance(value, list):
                    print(f"{key}: {', '.join(value[:3])}")
                else:
                    print(f"{key}: {value}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        statistics = scraped_data.get('statistics', {})
        if statistics:
            print(f"\nüìä STATISTICS PREVIEW")
            print("-" * 30)
            for key, value in list(statistics.items())[:2]:
                if isinstance(value, list):
                    print(f"{key}: {len(value)} items")
                    for item in value[:3]:
                        if isinstance(item, list):
                            print(f"   {' | '.join(item)}")
                        else:
                            print(f"   {item}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        predictions = scraped_data.get('predictions', {})
        if predictions:
            print(f"\nüéØ PREDICTIONS FOUND")
            print("-" * 30)
            for key, value in predictions.items():
                if isinstance(value, list):
                    print(f"{key}: {', '.join(value[:5])}")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
        final_report = {
            'match_id': match_id,
            'scraped_data': scraped_data,
            'analysis': analysis,
            'generated_at': datetime.now().isoformat()
        }
        
        with open(f'goal5_match_{match_id}_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ Complete analysis saved to goal5_match_{match_id}_analysis.json")
        print(f"\nüöÄ SCRAPING COMPLETED!")
        
    else:
        print(f"‚ùå Failed to scrape data from Goal5.co")

if __name__ == "__main__":
    main()
